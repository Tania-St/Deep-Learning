# Глубокое обучение: сборник лабораторных работ

Репозиторий содержит лабораторные работы, посвященные различным аспектам глубокого обучения.

## Лабораторная работа 1: Сверточные нейронные сети (CNN)

### Часть 1: Введение в CNN для классификации изображений
- Изучение архитектуры CNN (сверточные слои, пулинг, активационные функции)
- Подготовка датасета CIFAR-10
- Определение архитектуры сети
- Обучение модели и оценка точности

### Часть 2: Классификация с предобработкой и аугментацией данных
- Работа с датасетами CIFAR-10, Fashion-MNIST и SVHN
- Применение методов аугментации (отражения, повороты, яркость)
- Сравнение влияния аугментации на обобщающую способность

### Часть 3: Оптимизация архитектуры CNN
- Эксперименты с количеством/размером фильтров
- Использование Batch Normalization и Dropout
- Анализ влияния архитектурных параметров

### Часть 4: Оптимизация гиперпараметров
- Настройка learning rate и batch size
- Применение Cyclic Learning Rate и Adaptive Learning Rate
- Сравнение разных стратегий оптимизации


## Лабораторная работа 2: Архитектура трансформеров и NLP

### Основные этапы:
1. Подготовка текстового корпуса (очистка, токенизация)
2. Изучение компонентов трансформера:
   - Self-Attention и Multi-Head Attention
   - Позиционное кодирование
   - Feed-Forward Network
3. Создание модели на основе трансформера (GPT/BERT)
4. Обучение с разными оптимизаторами (Adam, SGD, RMSProp)
5. Оценка с помощью метрик:
   - Perplexity
   - BLEU, ROUGE
6. Разработка чат-бота для интерактивного тестирования

## Лабораторная работа 3: Прогнозирование медицинских исходов

### Решаемые задачи:
- Прогнозирование прогрессирования болезни (ВБП)
- Оценка контроля заболевания (КЗ)
- Определение объективного ответа
- Анализ токсичности 3-4 уровня

### Основные этапы:
1. Предобработка медицинских данных
2. Построение модели глубокого обучения для бинарной классификации
3. Обучение с динамическим learning rate
4. Оценка метрик (Accuracy, Precision, Recall, F1, ROC-AUC)
5. Интерпретация с помощью:
   - Grad-CAM
   - SHAP-анализа
   - Рекурсивного исключения признаков
6. Сравнение с Decision Tree

## Лабораторная работа 4: Вариационные автокодировщики (VAE)

### Основные этапы:
1. Подготовка данных (MNIST)
2. Изучение архитектуры VAE:
   - Энкодер/декодер
   - Латентное пространство
   - Функция потерь (KL-дивергенция)
3. Реализация классического AE и VAE
4. Сравнение с PCA
5. Генерация новых данных:
   - Случайные точки в латентном пространстве
   - Интерполяция между точками
6. Сравнение VAE и GAN
