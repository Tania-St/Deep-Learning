## Описание лабораторной работы
Лабораторная работа посвящена изучению архитектуры трансформеров, их применению в задачах обработки естественного языка (NLP) и созданию модели на основе LLM (Large Language Model). В рамках работы исследуются механизмы внимания (Self-Attention, Multi-Head Attention), обучение и тонкая настройка (fine-tuning) модели, а также оценка её производительности.

## Выполненные этапы

### 1. Введение и подготовка данных
- Подготовка текстового корпуса для обучения модели.
- Разделение данных на тренировочный, валидационный и тестовый наборы.
- Очистка данных:
  - Удаление пунктуации.
  - Удаление стоп-слов.
  - Лемматизация.
- Токенизация текста и преобразование в числовое представление.

### 2. Изучение архитектуры трансформера
- Разбор основных компонентов:
  - **Self-Attention** и **Multi-Head Attention**.
  - **Feed-Forward Network**.
  - Позиционное кодирование и остаточные связи.
- Сравнение трансформеров с RNN и LSTM: ключевые преимущества.

### 3. Создание модели на основе трансформера
- Инициализация модели (например, GPT или BERT) для fine-tuning.
- Настройка гиперпараметров:
  - Количество слоёв.
  - Размерность эмбеддингов.
  - Число эпох.
  - Скорость обучения (learning rate).

### 4. Обучение модели
- Обучение трансформера на подготовленных данных.
- Исследование влияния оптимизаторов:
  - Adam
  - SGD
  - RMSProp
- Использование **learning rate scheduler** для адаптивного изменения скорости обучения.

### 5. Оценка модели
- Метрики оценки:
  - **Perplexity** (уровень уверенности модели).
  - **BLEU** и **ROUGE** (качество генерации текста).
- Сравнение результатов при разных оптимизаторах и schedulers.

### 6. Интерактивное тестирование модели
- Разработка интерфейса для взаимодействия с моделью.
- Реализация чат-бота, отвечающего на запросы пользователей.
- Настройка параметров генерации:
  - `max_length`
  - `top_k`
  - `top_p`

### 7. Анализ результатов
- Эксперименты с гиперпараметрами и методами обучения.
- Построение графиков:
  - Потери (loss) vs эпохи.
  - Perplexity vs эпохи.
- Ручная оценка релевантности сгенерированных ответов.